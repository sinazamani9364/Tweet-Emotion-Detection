{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lLl5AjfevIO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import json\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filename):\n",
        "  with open(filename) as file:\n",
        "      data = json.load(file)\n",
        "  return data"
      ],
      "metadata": {
        "id": "QFBh9e-ke1Qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_result_to_csv(result, filename):\n",
        "  with open(filename, 'w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "    writer.writerow(list(result.keys()))\n",
        "    writer.writerow(list(result.values()))"
      ],
      "metadata": {
        "id": "Sds3bvuPkiOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data(result, filename):\n",
        "  x_data = []\n",
        "  y_data = []\n",
        "\n",
        "  for item in result:\n",
        "    x_data.append(item)\n",
        "    y_data.append(result[item])\n",
        "\n",
        "  plt.plot(x_data, y_data)\n",
        "  plt.xlabel('Word')\n",
        "  plt.ylabel('Count')\n",
        "  plt.title('Histogram')\n",
        "\n",
        "  plt.savefig(filename)"
      ],
      "metadata": {
        "id": "co3gafpJTY-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tweets = load_data('all_tweets_preprocessed.json')\n",
        "sadness_tweets = load_data('sadness_tweets.json')\n",
        "happiness_tweets = load_data('happiness_tweets.json')\n",
        "fear_tweets = load_data('fear_tweets.json')\n",
        "anger_tweets = load_data('anger_tweets.json')\n",
        "disgust_tweets = load_data('disgust_tweets.json')\n",
        "surprise_tweets = load_data('surprise_tweets.json')\n",
        "neutral_tweets = load_data('neutral_tweets.json')"
      ],
      "metadata": {
        "id": "AMc0v242kXVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_count = {'Sadness' : len(sadness_tweets), 'Happiness' : len(happiness_tweets), 'Fear' : len(fear_tweets), 'Anger' : len(anger_tweets), 'Disgust' : len(disgust_tweets), 'Surprise' : len(surprise_tweets), 'Neutral' : len(neutral_tweets), 'Total' : len(all_tweets)}\n",
        "write_result_to_csv(data_count, 'tweet_count.csv')"
      ],
      "metadata": {
        "id": "hYzIT2JRkYrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_sentences(tweet_set):\n",
        "  sent_count = 0\n",
        "  for idx in tweet_set.keys():\n",
        "    sent_count += len(tweet_set[idx]['sentence_broken'])\n",
        "  return sent_count\n",
        "\n",
        "sentence_count = {'Sadness' : count_sentences(sadness_tweets), 'Happiness' : count_sentences(happiness_tweets), 'Fear' : count_sentences(fear_tweets), 'Anger' : count_sentences(anger_tweets), 'Disgust' : count_sentences(disgust_tweets), 'Surprise' : count_sentences(surprise_tweets), 'Neutral' : count_sentences(neutral_tweets), 'Total' : count_sentences(all_tweets)}\n",
        "\n",
        "write_result_to_csv(sentence_count, 'sentence_count.csv')"
      ],
      "metadata": {
        "id": "jbwSQfsN0A58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_words(tweet_set):\n",
        "  word_count = 0\n",
        "  for idx in tweet_set.keys():\n",
        "    word_count += len(tweet_set[idx]['word_broken'])\n",
        "  return word_count\n",
        "\n",
        "word_count = {'Sadness' : count_words(sadness_tweets), 'Happiness' : count_words(happiness_tweets), 'Fear' : count_words(fear_tweets), 'Anger' : count_words(anger_tweets), 'Disgust' : count_words(disgust_tweets), 'Surprise' : count_words(surprise_tweets), 'Neutral' : count_words(neutral_tweets), 'Total' : count_words(all_tweets)}\n",
        "\n",
        "write_result_to_csv(word_count, 'word_count.csv')"
      ],
      "metadata": {
        "id": "huZQDFK-Bxw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_unique_words(tweet_set):\n",
        "  unique_words = []\n",
        "  for idx in tweet_set.keys():\n",
        "    unique_words.extend(tweet_set[idx]['word_broken'])\n",
        "  unique_words = list(set(unique_words))\n",
        "\n",
        "  return len(unique_words)\n",
        "\n",
        "unique_word_count = {'Sadness' : count_unique_words(sadness_tweets), 'Happiness' : count_unique_words(happiness_tweets), 'Fear' : count_unique_words(fear_tweets), 'Anger' : count_unique_words(anger_tweets), 'Disgust' : count_unique_words(disgust_tweets), 'Surprise' : count_unique_words(surprise_tweets), 'Neutral' : count_unique_words(neutral_tweets), 'Total' : count_unique_words(all_tweets)}\n",
        "\n",
        "write_result_to_csv(unique_word_count, 'unique_word_count.csv')"
      ],
      "metadata": {
        "id": "kUR4TZL-L6q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_common_words():\n",
        "  unique_words = []\n",
        "  for idx in all_tweets.keys():\n",
        "    unique_words.extend(all_tweets[idx]['word_broken'])\n",
        "  unique_words = list(set(unique_words))\n",
        "\n",
        "  unique_words_sadness = []\n",
        "  for idx in sadness_tweets.keys():\n",
        "    unique_words_sadness.extend(sadness_tweets[idx]['word_broken'])\n",
        "  unique_words_sadness = list(set(unique_words_sadness))\n",
        "\n",
        "  unique_words_happiness = []\n",
        "  for idx in happiness_tweets.keys():\n",
        "    unique_words_happiness.extend(happiness_tweets[idx]['word_broken'])\n",
        "  unique_words_happiness = list(set(unique_words_happiness))\n",
        "\n",
        "  unique_words_fear = []\n",
        "  for idx in fear_tweets.keys():\n",
        "    unique_words_fear.extend(fear_tweets[idx]['word_broken'])\n",
        "  unique_words_fear = list(set(unique_words_fear))\n",
        "\n",
        "  unique_words_anger = []\n",
        "  for idx in anger_tweets.keys():\n",
        "    unique_words_anger.extend(anger_tweets[idx]['word_broken'])\n",
        "  unique_words_anger = list(set(unique_words_anger))\n",
        "\n",
        "  unique_words_disgust = []\n",
        "  for idx in disgust_tweets.keys():\n",
        "    unique_words_disgust.extend(disgust_tweets[idx]['word_broken'])\n",
        "  unique_words_disgust = list(set(unique_words_disgust))\n",
        "\n",
        "  unique_words_surprise = []\n",
        "  for idx in surprise_tweets.keys():\n",
        "    unique_words_surprise.extend(surprise_tweets[idx]['word_broken'])\n",
        "  unique_words_surprise = list(set(unique_words_surprise))\n",
        "\n",
        "  unique_words_neutral = []\n",
        "  for idx in neutral_tweets.keys():\n",
        "    unique_words_neutral.extend(neutral_tweets[idx]['word_broken'])\n",
        "  unique_words_neutral = list(set(unique_words_neutral))\n",
        "\n",
        "  common_words = 0\n",
        "  for word in unique_words:\n",
        "    if word in unique_words_sadness and \\\n",
        "     word in unique_words_happiness and \\\n",
        "     word in unique_words_fear and \\\n",
        "     word in unique_words_anger and \\\n",
        "     word in unique_words_disgust and \\\n",
        "     word in unique_words_surprise and \\\n",
        "     word in unique_words_neutral:\n",
        "      common_words += 1\n",
        "\n",
        "  return common_words, len(unique_words) - common_words\n",
        "\n",
        "common, uncommon = count_common_words()\n",
        "\n",
        "common_and_uncommon_word_count = {'Common' : common, 'Uncommon' : uncommon}\n",
        "\n",
        "write_result_to_csv(common_and_uncommon_word_count, 'common_and_uncommon_word_count.csv')"
      ],
      "metadata": {
        "id": "9iLK3N8_Nox2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_word_count_histogram(tweet_set):\n",
        "  unique_words = {}\n",
        "  print(type(tweet_set), 1)\n",
        "  print(tweet_set)\n",
        "  for idx in tweet_set.keys():\n",
        "    for word in tweet_set[idx]['word_broken']:\n",
        "      if word != ' ':\n",
        "        if word in unique_words:\n",
        "          unique_words[word] += 1\n",
        "        else:\n",
        "          unique_words[word] = 1\n",
        "\n",
        "  unique_words = dict(sorted(unique_words.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "  return unique_words"
      ],
      "metadata": {
        "id": "Rr4lC_LrW8yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_docs_words():\n",
        "  return {'Sadness' : list(unique_word_count_histogram(sadness_tweets).keys()), 'Happiness' : list(unique_word_count_histogram(happiness_tweets).keys()), 'Anger' : list(unique_word_count_histogram(anger_tweets).keys()), 'Fear' : list(unique_word_count_histogram(fear_tweets).keys()), 'Disgust' : list(unique_word_count_histogram(disgust_tweets).keys()), 'Surprise' : list(unique_word_count_histogram(surprise_tweets).keys()), 'Neutral' : list(unique_word_count_histogram(neutral_tweets).keys())}"
      ],
      "metadata": {
        "id": "KJiVZPCxaz8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TermFrequency(document_words_count):\n",
        "    tf = {}\n",
        "    sum_doc = sum(document_words_count.values())\n",
        "    for w in document_words_count:\n",
        "        tf[w] = document_words_count[w] / sum_doc\n",
        "    return tf\n",
        "\n",
        "def InverseDocumentFrequency(docs_words, target_labels_words):\n",
        "    idf = {}\n",
        "    N = len(docs_words)\n",
        "    for w in target_labels_words:\n",
        "        d = 0\n",
        "        for doc in docs_words:\n",
        "            if w in docs_words[doc]:\n",
        "                d += 1\n",
        "        idf[w] = math.log2(N / d)\n",
        "    return idf\n",
        "\n",
        "def Tf_Idf(tf, idf):\n",
        "    tfidf = {}\n",
        "    for w in tf:\n",
        "        tfidf[w] = round(tf[w] * idf[w], 4)\n",
        "\n",
        "    tfidf = dict(sorted(tfidf.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "    return dict(itertools.islice(tfidf.items(), 8))\n",
        "\n",
        "write_result_to_csv(Tf_Idf(TermFrequency(unique_word_count_histogram(sadness_tweets)), InverseDocumentFrequency(get_docs_words(), list(unique_word_count_histogram(all_tweets).keys()))), 'sadness_tfidf.csv')\n",
        "write_result_to_csv(Tf_Idf(TermFrequency(unique_word_count_histogram(happiness_tweets)), InverseDocumentFrequency(get_docs_words(), list(unique_word_count_histogram(all_tweets).keys()))), 'happiness_tfidf.csv')\n",
        "write_result_to_csv(Tf_Idf(TermFrequency(unique_word_count_histogram(anger_tweets)), InverseDocumentFrequency(get_docs_words(), list(unique_word_count_histogram(all_tweets).keys()))), 'anger_tfidf.csv')\n",
        "write_result_to_csv(Tf_Idf(TermFrequency(unique_word_count_histogram(fear_tweets)), InverseDocumentFrequency(get_docs_words(), list(unique_word_count_histogram(all_tweets).keys()))), 'fear_tfidf.csv')\n",
        "write_result_to_csv(Tf_Idf(TermFrequency(unique_word_count_histogram(disgust_tweets)), InverseDocumentFrequency(get_docs_words(), list(unique_word_count_histogram(all_tweets).keys()))), 'disgust_tfidf.csv')\n",
        "write_result_to_csv(Tf_Idf(TermFrequency(unique_word_count_histogram(surprise_tweets)), InverseDocumentFrequency(get_docs_words(), list(unique_word_count_histogram(all_tweets).keys()))), 'surprise_tfidf.csv')\n",
        "write_result_to_csv(Tf_Idf(TermFrequency(unique_word_count_histogram(neutral_tweets)), InverseDocumentFrequency(get_docs_words(), list(unique_word_count_histogram(all_tweets).keys()))), 'neutral_tfidf.csv')"
      ],
      "metadata": {
        "id": "H90BsfHoVfup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data(dict(itertools.islice(unique_word_count_histogram(all_tweets).items(), 10)), 'top_unique_words.png')"
      ],
      "metadata": {
        "id": "vINIUK2lPjR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}